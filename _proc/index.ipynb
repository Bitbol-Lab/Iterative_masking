{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: index.html\n",
    "title: How to use\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Clone this repository on your local machine by running:\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:Bitbol-Lab/Iterative_masking.git\n",
    "```\n",
    "and move inside the root folder.\n",
    "One can the use directly the functions from the cloned repository (in the folder `Iterative_masking`) or install it with an editable install running:\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "We recommend creating and activating a dedicated ``conda`` or ``virtualenv`` Python virtual environment.\n",
    "\n",
    "## Requirements\n",
    "In order to use the functions, the following python packages are required:\n",
    "\n",
    "- numpy\n",
    "- scipy\n",
    "- numba\n",
    "- fastcore\n",
    "- biopython\n",
    "- esm==0.4.0\n",
    "- pytorch\n",
    "\n",
    "It is also required to use a GPU (with cuda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IM_MSA_Transformer`: Class with different functions used to generate new MSAs with the iterative masking procedure\n",
    "\n",
    "`gen_MSAs`: example function (with parser) that can be used to generate and save new sequences directly from the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on how to use `gen_MSAs` to replicate the results of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "gen_MSAs(filepath=\"examples\",\n",
    "         filename=[\"PF00072.fasta\"],\n",
    "         new_dir=\"results\",\n",
    "         pdf=False,\n",
    "         T=1,\n",
    "         sample_all=False,\n",
    "         Iters=200,\n",
    "         pmask=0.1,\n",
    "         num=[600],\n",
    "         depth=1,\n",
    "         generate=False,\n",
    "         print_all=False,\n",
    "         range_vals=False,\n",
    "         phylo_w=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples to generate new MSAs in a fast way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "filename = \"PF00072.fasta\"\n",
    "filepath = \"examples\"\n",
    "pmask = 0.1\n",
    "iterations = 20\n",
    "\n",
    "print('Tokenize')\n",
    "IM_class = IM_MSA_Transformer(p_mask=pmask, filename=[filename], num=[-1], filepath=filepath)\n",
    "idx_list = IM_class.idx_list\n",
    "tokenized_msa = IM_class.msa_batch_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate full MSA (mask all sequences and iterate)\n",
    "- If `use_pdf`=True, generate tokens by sampling from the logits at temperature `T`.\n",
    "- If `save_all`=True, then the first dimension of generated_tokens is the number of iterations.\n",
    "- If `rand_perm`=True, then the sequence order is shuffled at every iteration (and shuffled back at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "msa_tokens = tokenized_msa[:,:200]\n",
    "# If use_pdf=True, generate tokens by sampling from the logits at temperature T\n",
    "# If save_all=True, then the first dimension of generated_tokens is the number of iterations\n",
    "# If rand_perm=True, then the sequence order is shuffled at every iteration (and shuffled back at the end)\n",
    "generated_tokens = IM_class.generate_all_msa(msa_tokens, iterations, use_pdf=False, T=1, save_all=True, rand_perm=True)\n",
    "generated_tokens = IM_class.print_tokens(generated_tokens)\n",
    "print(\"Shape of the tokenized generated sequences: \", generated_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MSA with fixed context (mask all but context)\n",
    "- If `use_pdf`=True, generate tokens by sampling from the logits at temperature `T`.\n",
    "- If `save_all`=True, then the first dimension of generated_tokens is the number of iterations.\n",
    "- If `rand_perm`=True, then the sequence order is shuffled at every iteration (and shuffled back at the end).\n",
    "- If `use_rnd_ctx`=False, then the context is `all_context` and it's the same at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ancestor = tokenized_msa[:,:10]\n",
    "all_context = tokenized_msa[:,10:210]\n",
    "\n",
    "generated_tokens = IM_class.generate_with_context_msa(ancestor, iterations, use_pdf=False, T=1, all_context=all_context,\n",
    "                                                      use_rnd_ctx=False, save_all=True, rand_perm=True)\n",
    "generated_tokens = IM_class.print_tokens(generated_tokens)\n",
    "print(\"Shape of the tokenized generated sequences: \", generated_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MSA with variable context (mask all but context)\n",
    "- If `use_pdf`=True, generate tokens by sampling from the logits at temperature `T`.\n",
    "- If `save_all`=True, then the first dimension of generated_tokens is the number of iterations.\n",
    "- If `rand_perm`=True, then the sequence order is shuffled at every iteration (and shuffled back at the end).\n",
    "- If `use_rnd_ctx`=True, then the context is a different sub-MSA sampled at each iteration from the full MSA (the entire MSA is given as first entry of `all_context`, the depth of the sub-MSA is given by the second entry of `all_context`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ancestor = tokenized_msa[:,:10]\n",
    "all_context = (tokenized_msa[:,10:], 200)\n",
    "\n",
    "generated_tokens = IM_class.generate_with_context_msa(ancestor, iterations, use_pdf=False, T=1, all_context=all_context,\n",
    "                                                      use_rnd_ctx=True, use_two_msas=False, mode=\"same\", save_all=True, rand_perm=True)\n",
    "generated_tokens = IM_class.print_tokens(generated_tokens)\n",
    "# If save_all=True, then the first dimension of generated_tokens is the number of iterations\n",
    "print(\"Shape of the tokenized generated sequences: \", generated_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to sample sequences from two different MSAs separately you can use the following parameters:\n",
    "- `use_rnd_ctx`=True, same as before\n",
    "- `use_two_msas`=True, if you want to sample from two different MSAs given as a tuple in the first entry of `all_context` (the second entry of `all_context` is the depth of each sub-MSA).\n",
    "- `mode`, is the sampling mode, if `mode`=\"same\" then the same number of sequences is sampled from each MSA, if `mode`=\"ratio\" then it samples a number of sequences from each MSA proportional to the current iteration, starts with all sequences from the first MSA and no sequences from the second MSA, ends with no sequences from the first MSA and all sequences from the second MSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ancestor = tokenized_msa[:,:10]\n",
    "all_context = ((tokenized_msa[:,10:1000], tokenized_msa[:,1000:]), 200)\n",
    "\n",
    "generated_tokens = IM_class.generate_with_context_msa(ancestor, iterations, use_pdf=False, T=1, all_context=all_context,\n",
    "                                                      use_rnd_ctx=True, use_two_msas=True, mode=\"same\", save_all=True, rand_perm=True)\n",
    "generated_tokens = IM_class.print_tokens(generated_tokens)\n",
    "# If save_all=True, then the first dimension of generated_tokens is the number of iterations\n",
    "print(\"Shape of the tokenized generated sequences: \", generated_tokens.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
