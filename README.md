# How to use

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Getting started

Clone this repository on your local machine by running:

``` bash
git clone git@github.com:Bitbol-Lab/Iterative_masking.git
```

and move inside the root folder. One can the use directly the functions
from the cloned repository (in the folder `Iterative_masking`) or
install it with an editable install running:

``` bash
pip install -e .
```

We recommend creating and activating a dedicated `conda` or `virtualenv`
Python virtual environment.

## Requirements

In order to use the functions, the following python packages are
required:

- numpy
- scipy
- numba
- fastcore
- biopython
- esm==0.4.0
- pytorch

It is also required to use a GPU (with cuda).

`IM_MSA_Transformer`: Class with different functions used to generate
new MSAs with the iterative masking procedure

`gen_MSAs`: example function (with parser) that can be used to generate
and save new sequences directly from the terminal.

## Example on how to use `gen_MSAs` to replicate the results of the paper

``` python
gen_MSAs(filepath="examples",
         filename=["PF00072.fasta"],
         new_dir="results",
         pdf=False,
         T=1,
         sample_all=False,
         Iters=200,
         pmask=0.1,
         num=[600],
         depth=1,
         generate=False,
         print_all=False,
         range_vals=False,
         phylo_w=False)
```

## Some examples to generate new MSAs in a fast way

``` python
filename = "PF00072.fasta"
filepath = "examples"
pmask = 0.1
iterations = 200

print('Tokenize')
IM_class = IM_MSA_Transformer(p_mask=pmask, filename=[filename], num=[-1], filepath=filepath)
idx_list = IM_class.idx_list
tokenized_msa = IM_class.msa_batch_tokens
```

### Generate full MSA (mask all sequences and iterate)

``` python
msa_tokens = tokenized_msa[:,:200]

generated_tokens = IM_class.generate_all_msa(msa_tokens, iterations, use_pdf=False, T=1, save_all=True)
generated_tokens = IM_class.print_tokens(generated_tokens)
# If save_all=True, then the first dimension of generated_tokens is the number of iterations
print("Shape of the tokenized generated sequences: ", generated_tokens.shape)
```

### Generate MSA with fixed context (mask all but context)

``` python
ancestor = tokenized_msa[:,:10]
all_context = tokenized_msa[:,10:210]

generated_tokens = IM_class.generate_with_context_msa(ancestor, iterations, use_pdf=False, T=1, all_context=all_context, use_rnd_ctx=False, save_all=True)
generated_tokens = IM_class.print_tokens(generated_tokens)
# If save_all=True, then the first dimension of generated_tokens is the number of iterations
print("Shape of the tokenized generated sequences: ", generated_tokens.shape)
```

### Generate MSA with variable context (mask all but context)

``` python
ancestor = tokenized_msa[:,:10]
all_context = (tokenized_msa[:,10:], 200)

generated_tokens = IM_class.generate_with_context_msa(ancestor, iterations, use_pdf=False, T=1, all_context=all_context, use_rnd_ctx=True, save_all=True)
generated_tokens = IM_class.print_tokens(generated_tokens)
# If save_all=True, then the first dimension of generated_tokens is the number of iterations
print("Shape of the tokenized generated sequences: ", generated_tokens.shape)
```
